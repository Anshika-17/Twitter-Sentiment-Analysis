{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# <xaiArtifact artifact_id=\"7fca53d3-eeb9-4e51-9992-096280baf133\" title=\"train_sentiment_model.ipynb\" contentType=\"text/x-python\">\n",
    "# Set memory allocation configuration for PyTorch\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Install required packages\n",
    "!pip install transformers datasets pandas numpy torch accelerate\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    MAX_LENGTH = 128\n",
    "    MODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment'  # Twitter-specific model\n",
    "    BASE_BATCH_SIZE = 8\n",
    "    NUM_EPOCHS = 3\n",
    "    LEARNING_RATE = 2e-5\n",
    "    WARMUP_STEPS = 500\n",
    "    LOGGING_STEPS = 100\n",
    "    SAVE_STEPS = 1000\n",
    "    EARLY_STOPPING_PATIENCE = 2\n",
    "    EARLY_STOPPING_THRESHOLD = 0.01\n",
    "\n",
    "# Enhanced data loading with validation for headerless CSV\n",
    "def load_and_validate_data(file_path, has_headers=True):\n",
    "    \"\"\"Load and validate dataset with comprehensive checks\"\"\"\n",
    "    try:\n",
    "        # Try reading with different encodings if needed\n",
    "        try:\n",
    "            if has_headers:\n",
    "                df = pd.read_csv(file_path)\n",
    "            else:\n",
    "                df = pd.read_csv(file_path, header=None)\n",
    "                # Assign column names for headerless CSV\n",
    "                df.columns = ['Tweet ID', 'entity', 'sentiment', 'Tweet content']\n",
    "        except UnicodeDecodeError:\n",
    "            if has_headers:\n",
    "                df = pd.read_csv(file_path, encoding='latin1')\n",
    "            else:\n",
    "                df = pd.read_csv(file_path, header=None, encoding='latin1')\n",
    "                df.columns = ['Tweet ID', 'entity', 'sentiment', 'Tweet content']\n",
    "            \n",
    "        # Validate required columns\n",
    "        required_columns = {'Tweet ID', 'entity', 'sentiment', 'Tweet content'}\n",
    "        if not required_columns.issubset(df.columns):\n",
    "            missing = required_columns - set(df.columns)\n",
    "            raise ValueError(f\"Missing required columns: {missing}\")\n",
    "            \n",
    "        # Filter and validate sentiments\n",
    "        valid_sentiments = {'Positive', 'Negative', 'Neutral', 'Irrelevant'}\n",
    "        invalid_sentiments = set(df['sentiment'].unique()) - valid_sentiments\n",
    "        if invalid_sentiments:\n",
    "            raise ValueError(f\"Invalid sentiment values found: {invalid_sentiments}\")\n",
    "            \n",
    "        # Filter out irrelevant and missing data\n",
    "        df = df[df['sentiment'] != 'Irrelevant']\n",
    "        df = df.dropna(subset=['Tweet content', 'sentiment'])\n",
    "        \n",
    "        # Remove duplicates\n",
    "        df = df.drop_duplicates(subset=['Tweet ID'], keep='first')\n",
    "        \n",
    "        # Map sentiments\n",
    "        sentiment_map = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "        df['label'] = df['sentiment'].map(sentiment_map)\n",
    "        \n",
    "        # Validate label mapping\n",
    "        if df['label'].isna().any():\n",
    "            raise ValueError(\"Some sentiments couldn't be mapped to labels\")\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Memory-optimized dataset class\n",
    "class OptimizedTweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = {\n",
    "            'input_ids': encodings['input_ids'],\n",
    "            'attention_mask': encodings['attention_mask']\n",
    "        }\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Enhanced training function with Twitter-RoBERTa model\n",
    "def train_model(train_df, output_dir=\"sentiment_model\"):\n",
    "    try:\n",
    "        # Clear memory before starting\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Initialize device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Initialize tokenizer and model with Twitter-RoBERTa\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            Config.MODEL_NAME, \n",
    "            num_labels=3\n",
    "        ).to(device)\n",
    "        \n",
    "        # Enable gradient checkpointing to save memory\n",
    "        model.gradient_checkpointing_enable()\n",
    "        \n",
    "        # Convert to HuggingFace Dataset\n",
    "        dataset = Dataset.from_pandas(train_df)\n",
    "        \n",
    "        # Tokenization function\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(\n",
    "                examples['Tweet content'],\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=Config.MAX_LENGTH\n",
    "            )\n",
    "            \n",
    "        # Tokenize dataset\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=Config.BASE_BATCH_SIZE\n",
    "        )\n",
    "        \n",
    "        # Split into train and validation (10% for validation)\n",
    "        train_val_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "        train_dataset = train_val_split['train']\n",
    "        val_dataset = train_val_split['test']\n",
    "        \n",
    "        # Compute metrics function\n",
    "        def compute_metrics(p):\n",
    "            predictions, labels = p\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "            \n",
    "            return {\n",
    "                'accuracy': accuracy_score(labels, predictions),\n",
    "                'f1': f1_score(labels, predictions, average='weighted')\n",
    "            }\n",
    "            \n",
    "        # Training arguments with memory optimizations\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            num_train_epochs=Config.NUM_EPOCHS,\n",
    "            per_device_train_batch_size=Config.BASE_BATCH_SIZE,\n",
    "            per_device_eval_batch_size=Config.BASE_BATCH_SIZE,\n",
    "            gradient_accumulation_steps=4,  # Accumulate gradients over 4 steps\n",
    "            learning_rate=Config.LEARNING_RATE,\n",
    "            warmup_steps=Config.WARMUP_STEPS,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=Config.LOGGING_STEPS,\n",
    "            eval_strategy=\"steps\",  # Use eval_strategy instead of evaluation_strategy\n",
    "            eval_steps=Config.LOGGING_STEPS,\n",
    "            save_steps=Config.SAVE_STEPS,\n",
    "            save_total_limit=1,  # Save only best model to save disk space\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            fp16=True,  # Use mixed precision training\n",
    "            gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    "            dataloader_num_workers=2,  # Parallel data loading\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "        \n",
    "        # Initialize Trainer with early stopping\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[\n",
    "                EarlyStoppingCallback(\n",
    "                    early_stopping_patience=Config.EARLY_STOPPING_PATIENCE,\n",
    "                    early_stopping_threshold=Config.EARLY_STOPPING_THRESHOLD\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Clear memory before training\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Starting training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save best model\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        trainer.save_model(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Check GPU memory before starting\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Memory before loading data: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "            print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        \n",
    "        # Load data (update path to your dataset)\n",
    "        print(\"Loading and validating data...\")\n",
    "        train_df = load_and_validate_data(\"twitter_training.csv\", has_headers=False)\n",
    "        \n",
    "        # Train model\n",
    "        success = train_model(train_df)\n",
    "        \n",
    "        if success:\n",
    "            print(\"Training completed successfully!\")\n",
    "            # Zip model for download\n",
    "            !zip -r sentiment_model.zip sentiment_model\n",
    "            print(\"Model zipped and ready for download.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
